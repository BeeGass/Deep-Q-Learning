{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1XAl1oipf47tEcwI8+4QU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeeGassy/Deep-Q-Learning/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfsr21qloAQl"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from gym.wrappers import FrameStack\n",
        "from pdb import set_trace\n",
        "import random\n",
        "from tqdm import trange\n",
        "import atari_py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p746VD8tIlyz"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  #takes the # of frames stacked and the possible outputs (move right, left, etc)\n",
        "  def __init__(self, numberStacked, possibleOutputs):\n",
        "    super(Model, self).__init__()\n",
        "    hiddenKernels = 16\n",
        "\n",
        "    sizePostConvolution = 525824 #figure this out, depends on how we modify the env\n",
        "    self.conv1 = nn.Conv2d(numberStacked, hiddenKernels, 2)\n",
        "    self.rl = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(hiddenKernels, hiddenKernels, 2)\n",
        "    self.fc1 = nn.Linear(sizePostConvolution, possibleOutputs)\n",
        "\n",
        "  def forward(self, stackedState):\n",
        "    x = self.conv1(stackedState)\n",
        "    x = self.rl(x)\n",
        "    x = self.conv2(x)\n",
        "    print(x.size())\n",
        "    x = x.view(x.size(0), -1)\n",
        "    #x = x.view(-1, x.size()[1] * x.size()[2] * x.size()[3])\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPYqt0JBEtgR"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, size, numberStacked, possibleOutputs, gamma):\n",
        "    self.replay_buffer_size = size\n",
        "    self.replay_buffer_list = []\n",
        "    self.m = Model(numberStacked, possibleOutputs)\n",
        "    self.optimizer = optim.Adam(self.m.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    self.lossFn = torch.nn.MSELoss()\n",
        "    self.gamma = gamma\n",
        "    self.batch_size = 16\n",
        "\n",
        "  def action_value(self, input_state):\n",
        "    with torch.no_grad():\n",
        "        q_val = self.m(input_state)\n",
        "    action = torch.argmax(q_val)\n",
        "    return action\n",
        "\n",
        "  def sample_replay_buffer(self, batch_size):\n",
        "    #print(len(self.replay_buffer_list))\n",
        "    choices = np.random.choice(len(self.replay_buffer_list), batch_size)\n",
        "    perm = torch.tensor(choices)\n",
        "    idx = perm[:batch_size]\n",
        "    samples = np.array(self.replay_buffer_list)[idx]\n",
        "    return samples\n",
        "\n",
        "  def SGD(self):\n",
        "    mini_batch = self.sample_replay_buffer(self.batch_size)\n",
        "    for batch in mini_batch:\n",
        "        self.optimizer.zero_grad()\n",
        "        state, action, reward, next_state, done = batch\n",
        "        yj = reward\n",
        "        print(\"the reward\", yj)\n",
        "        if not done:\n",
        "          print(\"the type for next state:\", type(next_state))\n",
        "          q_val = self.m(next_state) \n",
        "          #best_predicted_action = torch.argmax(q_val)\n",
        "          #best_predicted_next_state, best_predicted_reward, done, _ = d.test_env.step(best_predicted_action)\n",
        "          print(\"q_val: \", q_val)\n",
        "          yj += self.gamma * q_val\n",
        "        \n",
        "        q_val = self.m(state)\n",
        "        predicted_action = torch.argmax(q_val)\n",
        "        predicted_next_state, best_reward, done, _ = d.test_env.step(predicted_action)\n",
        "        print(\"type of yj: \", type(yj))\n",
        "        print(\"type of best_reward: \", type(best_reward))\n",
        "        loss = self.lossFn(torch.tensor(best_reward, requires_grad=True), yj.detach())\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    return loss\n",
        "  \n",
        "  def addToReplay(self, newInput):\n",
        "    if len(self.replay_buffer_list) >= self.replay_buffer_size:#random eviction\n",
        "        toEvict = random.randint(0, self.rb_size)\n",
        "        del replay_buffer_list[toEvict]\n",
        "    self.replay_buffer_list.append(newInput)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EMBVNGvT7Nh"
      },
      "source": [
        "class DQN():\n",
        "  def __init__(self):\n",
        "    self.episodes = 400\n",
        "    self.time_in_episode = 1000000\n",
        "    self.epsilon = 0.7\n",
        "    self.possibleOutputs = 6\n",
        "    self.gamma = 0.01\n",
        "    self.rbSize = 100000\n",
        "    self.numberStacked = 4\n",
        "    self.height = 210\n",
        "    self.width = 160\n",
        "    self.agent = Agent(self.rbSize, self.numberStacked, self.possibleOutputs, self.gamma)\n",
        "    DEFAULT_ENV_NAME = 'PongNoFrameskip-v4'\n",
        "    self.test_env = gym.make(DEFAULT_ENV_NAME)\n",
        "    self.test_env = self.stack_frames(self.test_env, self.numberStacked)\n",
        "    \n",
        "  def initTransition(self):\n",
        "    state = self.test_env.reset()    \n",
        "    action = self.test_env.action_space.sample()\n",
        "    next_state, reward, _, _ = self.test_env.step(action)\n",
        "    transition = (state, action, next_state, reward)\n",
        "\n",
        "    return transition\n",
        "\n",
        "  def execute_action(self, input_action, state):\n",
        "    next_state, reward, done, _ = self.test_env.step(input_action)\n",
        "    transition = (state, input_action, reward, next_state, done)\n",
        "    self.agent.addToReplay((state, input_action, reward, next_state, done))\n",
        "\n",
        "    return transition\n",
        "\n",
        "  def preprocessing(self, input_next_state):\n",
        "    print(f'preprocess: {input_next_state.shape}')\n",
        "    \n",
        "    np_next_state = np.transpose(input_next_state, (0, 3, 1, 2))#batch h w color to batch color h w\n",
        "    print(f'preprocess: {np_next_state.shape}')\n",
        "\n",
        "    copy_next_state = np_next_state.copy()\n",
        "    torch_next_state = torch.tensor(copy_next_state, dtype=torch.float)\n",
        "    transform = T.Grayscale()\n",
        "    #transform = T.Compose([ T.Grayscale(), T.ToTensor(), T.ToPILImage()])\n",
        "    grey_scaled_next_state = transform(torch_next_state)\n",
        "    #print(f'grey_scaled_next_state: {grey_scaled_next_state.shape}')\n",
        "    grey_scaled_next_state = grey_scaled_next_state.view(1, self.numberStacked, self.height, self.width)\n",
        "    #print(f'grey_scaled_next_state: {grey_scaled_next_state.shape}')\n",
        "\n",
        "    return grey_scaled_next_state\n",
        "\n",
        "\n",
        "  #stack the frames of the states in group of 4. 4 Frames per stack\n",
        "  def stack_frames(self, input_env, stack_count):\n",
        "    enviroment = FrameStack(input_env, stack_count)\n",
        "\n",
        "    return enviroment\n",
        "  \n",
        "  def train(self):\n",
        "    for e in trange(self.episodes):\n",
        "      rewardVal = 0\n",
        "\n",
        "      #initialize episode and get first transition\n",
        "      initial_transition = self.initTransition()\n",
        "      state, action, next_state, reward = initial_transition\n",
        "\n",
        "      #preprocess data\n",
        "      grey_scaled_next_state = self.preprocessing(next_state)\n",
        "\n",
        "      for time_step in range(self.time_in_episode):\n",
        "        random_action_prob = random.uniform(0.0, 1.0)\n",
        "        if random_action_prob < self.epsilon:\n",
        "          action = self.test_env.action_space.sample()\n",
        "        else: \n",
        "          action = self.agent.action_value(grey_scaled_next_state)\n",
        "\n",
        "        #perform action for timestep\n",
        "        initialState = grey_scaled_next_state\n",
        "        print(action) \n",
        "        state, action, reward, next_state, done  = self.execute_action(action, grey_scaled_next_state)\n",
        "        grey_scaled_next_state = self.preprocessing(next_state)\n",
        "\n",
        "        #send all information into our replay buffer so we can test on it within SGD\n",
        "        self.agent.addToReplay((initialState, action, reward, grey_scaled_next_state, done))\n",
        "        self.agent.SGD()\n",
        "\n",
        "        #perform epsilon decay\n",
        "        epsilon_decay_rate = max((e - time_step) / e, 0)\n",
        "        epsilon -= epsilon_decay_rate\n",
        "        \n",
        "    #only render every 100 episodes\n",
        "    if e % 100 == 0 and e > 0:\n",
        "      self.test_env.render()    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkY_6X-TnRyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed8b47a4-afe1-4033-91df-563520049309"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  d = DQN()\n",
        "  DQN.train(d)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/400 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1, 6])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "  0%|          | 0/400 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preprocess: (4, 210, 160, 3)\n",
            "preprocess: (4, 3, 210, 160)\n",
            "5\n",
            "preprocess: (4, 210, 160, 3)\n",
            "preprocess: (4, 3, 210, 160)\n",
            "the reward 0.0\n",
            "the type for next state: <class 'torch.Tensor'>\n",
            "torch.Size([1, 16, 208, 158])\n",
            "q_val:  tensor([[ -4.8443,   2.6262, -12.3209,  -2.1878,  -7.9786,  -1.6866]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "torch.Size([1, 16, 208, 158])\n",
            "type of yj:  <class 'torch.Tensor'>\n",
            "type of best_reward:  <class 'float'>\n",
            "the reward 0.0\n",
            "the type for next state: <class 'torch.Tensor'>\n",
            "torch.Size([1, 16, 208, 158])\n",
            "q_val:  tensor([[ -4.8443,   2.6262, -12.3209,  -2.1878,  -7.9786,  -1.6866]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "torch.Size([1, 16, 208, 158])\n",
            "type of yj:  <class 'torch.Tensor'>\n",
            "type of best_reward:  <class 'float'>\n",
            "the reward 0.0\n",
            "the type for next state: <class 'torch.Tensor'>\n",
            "torch.Size([1, 16, 208, 158])\n",
            "q_val:  tensor([[ -4.8443,   2.6262, -12.3209,  -2.1878,  -7.9786,  -1.6866]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "torch.Size([1, 16, 208, 158])\n",
            "type of yj:  <class 'torch.Tensor'>\n",
            "type of best_reward:  <class 'float'>\n",
            "the reward 0.0\n",
            "the type for next state: <class 'gym.wrappers.frame_stack.LazyFrames'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a3332faf5af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-e8d2c1ae7059>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddToReplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitialState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrey_scaled_next_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mepsilon_decay_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilon_decay_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a25b440a5788>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the type for next state:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0mq_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m           \u001b[0;31m#best_predicted_action = torch.argmax(q_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0;31m#best_predicted_next_state, best_predicted_reward, done, _ = d.test_env.step(best_predicted_action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d52943b37f65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, stackedState)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackedState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstackedState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not LazyFrames"
          ]
        }
      ]
    }
  ]
}